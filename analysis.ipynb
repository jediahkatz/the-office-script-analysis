{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_characters = {\n",
    "    'michael', 'dwight', 'jim', 'andy', 'pam',                  # 10,000+ non-stopwords / 40,000+ total words\n",
    "    'angela', 'kevin', 'erin', 'oscar', 'ryan', 'darryl'        # 4,500+ non-stopwords / 11,000+ total words\n",
    "}\n",
    "minor_characters = {\n",
    "    'kelly', 'toby', 'jan', 'phyllis', 'nellie', 'robert',      # 2500+ non-stopwords\n",
    "    'gabe', 'stanley', 'meredith', 'holly', 'creed', 'david',\n",
    "    'deangelo', 'jo', 'karen', 'clark'                          # 1000+ non-stopwords\n",
    "    'roy', 'charles', 'pete', 'packer'                          # 500+ non-stopwords\n",
    "}\n",
    "aliases = {\n",
    "    'david wallace': 'david', 'daryl': 'darryl', 'todd packer': 'packer', 'todd': 'packer', 'robert california': 'robert'\n",
    "}\n",
    "word_substitutions = {\n",
    "    'oo': 'ooo', 'oooo': 'ooo', 'b*st*rd': 'bastard', 'f***ing': 'fucking', 'g*n': 'gun', 'sh**t': 'shoot', 'q*eer': 'queer'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from functools import reduce\n",
    "import re\n",
    "\n",
    "def get_ep_character_bows(script_lines):\n",
    "    \"\"\"Return a set of bags of words for this episode as a mapping from characters to words to counts.\"\"\"\n",
    "    bow = defaultdict(Counter)\n",
    "    for line in script_lines:\n",
    "        speaker, line = line.split('::')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        line = line.lower().replace('\\'', '')\n",
    "        # Get all non-stop words\n",
    "        stop_words = set(w for w in stopwords.words('english') \n",
    "            + ['us', 'get', 'like', 'thats', 'go', 'going', 'cant', 'oh', 'got', 'hey', 'would',\n",
    "               'whats', 'could', 'shall', 'gonna', 'okay', 'one', 'something', 'youre', 'dont',\n",
    "               'may', '00', 'im',]\n",
    "            + [c in 'abcdefghijklmnopqrstuvwxyz']\n",
    "            + [str(n) for n in range(20)]\n",
    "        )\n",
    "        # Special cases\n",
    "        words = [w.replace('_', ' ') for w in\n",
    "            re.findall(r'\\w+', \n",
    "                reduce(lambda acc, k: acc.replace(k, word_substitutions[k]), word_substitutions.keys(), line)\n",
    "            )]\n",
    "        bow[speaker]['total_count'] += len(words)\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "        bow[speaker].update(words)\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ep_total_word_count(script_lines):\n",
    "    return sum(len(re.findall(r'\\w+', line.split('::')[1])) for line in script_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_bows(bows):\n",
    "    \"\"\"Merge a collection of bags of words into one. Does not modify the input.\"\"\"\n",
    "    combined_bow = Counter()\n",
    "    for bow in bows:\n",
    "        combined_bow.update(bow)\n",
    "    return combined_bow\n",
    "\n",
    "def combine_character_bows(character_bows):\n",
    "    \"\"\"Merge a collection of mappings from characters to bags of words into one. Does not modify the input.\"\"\"\n",
    "    all_characters = set(k for bow in character_bows for k in bow.keys())\n",
    "    combined_bow_set = {c: combine_bows(bow_map[c] for bow_map in character_bows) for c in all_characters}\n",
    "    return defaultdict(Counter, combined_bow_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is a bad metric -- it just finds words that this character said once and nobody else ever said.\n",
    "def get_words_by_bayes(character_bows):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, p), where w is a word \n",
    "    and p is the probability that a random occurrence of the word was spoken by that character.\n",
    "    The lists are sorted by descending probability.\n",
    "    \"\"\"\n",
    "    # Compute the total number of occurrences for each word\n",
    "    combined_bow = combine_bows(character_bows.values())\n",
    "    # p(character | word) = [# times character says word] / [# occurrences of word]\n",
    "    words_by_bayes = {\n",
    "        c: sorted(\n",
    "            [(w, character_bows[c][w] / combined_bow[w]) for w in combined_bow.keys()],\n",
    "            key=lambda wp: wp[1],\n",
    "            reverse=True\n",
    "        ) for c in character_bows.keys()\n",
    "    }\n",
    "    return words_by_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "# Since we have very few characters, we want to weight idf highly (with a lower base)\n",
    "base = 1.2\n",
    "\n",
    "# I experimented with these parameters but ultimately decided they were more harmful than helpful on such a small dataset\n",
    "def get_words_by_tfidf(character_bows, character_set=None, sublinear_tf=False, filter_df_1=False):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, s), where w is a word \n",
    "    and s is the tf-idf score of that word for that character. The lists are sorted \n",
    "    by descending score.\n",
    "    \"\"\"\n",
    "    # If no explicit character set is passed, just use all characters\n",
    "    character_set = character_set or character_bows.keys()\n",
    "    # Compute the number of characters that said each word\n",
    "    all_words = set(k for bow in character_bows.values() for k in bow.keys())\n",
    "    n_chars = len(character_set)\n",
    "    idf = {\n",
    "        w: 1 + log(n_chars / (1 + df), base)\n",
    "        for w in all_words if (df := sum(character_bows[c][w] > 0 for c in character_set)) > (1 if filter_df_1 else 0)\n",
    "    }\n",
    "    # tf-idf = tf(w, c) / (1 + log(N / (1 + df(w))))\n",
    "    words_by_tfidf = {\n",
    "        c: sorted([\n",
    "                (w, ((1 + log(v)) if sublinear_tf else v) * idf[w]) \n",
    "                for w, v in character_bows[c].items() \n",
    "                if w != 'total_count' and (not filter_df_1 or w in idf)\n",
    "            ],\n",
    "            key=lambda ws: ws[1],\n",
    "            reverse=True\n",
    "        ) for c in character_set\n",
    "    }\n",
    "    return words_by_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_character_sentence_sentiments(script_lines, character_set=None):\n",
    "    \"\"\"\n",
    "    Return a mapping from each character to a list of all their sentences along with the\n",
    "    computed sentiment scores (positive, negative, neutral) for each sentence.\n",
    "    \"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    char_sentence_sentiments = defaultdict(list)\n",
    "    for line in script_lines:\n",
    "        speaker, line = line.split('::')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        if character_set and speaker not in character_set:\n",
    "            continue\n",
    "        \n",
    "        char_sentence_sentiments[speaker].extend(\n",
    "            (sid.polarity_scores(sentence), sentence) for sentence in tokenizer.tokenize(line)\n",
    "        )\n",
    "\n",
    "    return char_sentence_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_character_sentence_sentiments(target_map, added_map):\n",
    "    \"\"\"\n",
    "    Given two maps from characters to lists of sentence sentiments, combine all lists with the same key.\n",
    "    The first argument will be modified.\n",
    "    \"\"\"\n",
    "    for char in added_map.keys():\n",
    "        target_map[char].extend(added_map[char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_character_sentiment_frequencies(char_sentence_sentiments):\n",
    "    \"\"\"\n",
    "    Given a map from characters to lists of sentence sentiments, return a map\n",
    "    from characters to lists of the frequency of positive, negative, and neutral\n",
    "    sentiments.\n",
    "    \"\"\"\n",
    "    sentiment_freqs = defaultdict(lambda: defaultdict(int))\n",
    "    for char, sentiments in char_sentence_sentiments.items():\n",
    "        for (scores, _) in sentiments:\n",
    "            sentiment_freqs[char][\n",
    "                'pos' if scores['compound'] > 0.05 else 'neg' if scores['compound'] < -0.05 else 'neu'\n",
    "            ] += 1/len(sentiments)\n",
    "    return sentiment_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "season_bows_collection = defaultdict(list)\n",
    "season_bows = {}\n",
    "season_tfidf = {}\n",
    "season_tfidf_minor = {}\n",
    "season_bayes = {}\n",
    "total_word_count = {}\n",
    "character_sentence_sentiments = defaultdict(list)\n",
    "\n",
    "episodes = [ep.replace('.txt', '') for ep in listdir('transcripts/')]\n",
    "for season in range(1, 10):\n",
    "    for episode in (e for e in episodes if f'0{season}x' in e):\n",
    "        with open(f'transcripts/{episode}.txt') as f:\n",
    "            script_lines = f.readlines()\n",
    "            season_bows_collection[season].append(get_ep_character_bows(script_lines))\n",
    "            merge_character_sentence_sentiments(character_sentence_sentiments, get_character_sentence_sentiments(script_lines))\n",
    "            total_word_count[episode] = get_ep_total_word_count(script_lines)\n",
    "    season_bows[season] = combine_character_bows(season_bows_collection[season])\n",
    "    # It helps to compute tf-idf scores for major characters only (excluding minor characters) or\n",
    "    # else depth-of-character words like \"know\", \"want\", \"think\" are weighted too highly\n",
    "    season_tfidf[season] = get_words_by_tfidf(season_bows[season], character_set=major_characters)\n",
    "    season_tfidf_minor[season] = get_words_by_tfidf(season_bows[season], character_set=major_characters | minor_characters)\n",
    "    season_bayes[season] = get_words_by_bayes(season_bows[season])\n",
    "season_bows['all'] = combine_character_bows([bows for s in range(1, 10) for bows in season_bows_collection[s]])\n",
    "season_tfidf['all'] = get_words_by_tfidf(season_bows['all'], character_set=major_characters)\n",
    "season_tfidf_minor['all'] = get_words_by_tfidf(season_bows['all'], character_set=major_characters | minor_characters)\n",
    "season_bayes['all'] = get_words_by_bayes(season_bows['all'])\n",
    "character_sentiment_frequencies = get_character_sentiment_frequencies(character_sentence_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by total word count\n",
    "count_sorted = sorted(\n",
    "    ((char, count) \n",
    "        for char in season_bows['all'].keys()\n",
    "        if (count := season_bows['all'][char]['total_count'])),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dump\n",
    "\n",
    "words_json = defaultdict(dict)\n",
    "count_json = defaultdict(list)\n",
    "sentiment_json = defaultdict(list)\n",
    "for c in sorted(major_characters | minor_characters):\n",
    "    ct = c.title()\n",
    "    for season in ['all'] + list(range(1, 10)):\n",
    "        top_ten = [\n",
    "            w for (w, _) in (\n",
    "                season_tfidf[season][c][:10] if c in major_characters else season_tfidf_minor[season][c][:10]\n",
    "            )\n",
    "        ]\n",
    "        words_json[ct][season] = top_ten\n",
    "    words_json[ct]['word_count'] = season_bows['all'][c]['total_count']\n",
    "    words_json[ct]['neg_freq'] = character_sentiment_frequencies[c]['neg']\n",
    "    words_json[ct]['neu_freq'] = character_sentiment_frequencies[c]['neu']\n",
    "    words_json[ct]['pos_freq'] = character_sentiment_frequencies[c]['pos']\n",
    "    words_json[ct]['net_freq'] = character_sentiment_frequencies[c]['pos'] - character_sentiment_frequencies[c]['neg']\n",
    "\n",
    "for (c , wc) in count_sorted[:10]:\n",
    "    for season in range(1, 10):\n",
    "        count_json[f'S{season}'].append(season_bows[season][c]['total_count'])\n",
    "    count_json['characters'].append(c.title())\n",
    "\n",
    "# Arbitrarily selected list of characters to highlight for sentiment analysis\n",
    "for c in ['michael', 'dwight', 'jim', 'andy', 'pam', 'angela', 'kevin', 'robert', 'ryan', 'stanley']:\n",
    "    sentiment_json['characters'].append(c.title())\n",
    "    sentiment_json['pos'].append(character_sentiment_frequencies[c]['pos'])\n",
    "    sentiment_json['neg'].append(-character_sentiment_frequencies[c]['neg'])\n",
    "\n",
    "with open('results.json', 'w') as f:\n",
    "    dump([words_json, count_json, sentiment_json], f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('michael', 66048),\n",
       " ('dwight', 36328),\n",
       " ('jim', 26071),\n",
       " ('andy', 20997),\n",
       " ('pam', 20301),\n",
       " ('angela', 6419),\n",
       " ('kevin', 6123),\n",
       " ('erin', 5989),\n",
       " ('oscar', 5942),\n",
       " ('darryl', 5443),\n",
       " ('ryan', 5400),\n",
       " ('kelly', 4093),\n",
       " ('toby', 3519),\n",
       " ('jan', 3485),\n",
       " ('phyllis', 3482),\n",
       " ('nellie', 3215),\n",
       " ('robert', 2895),\n",
       " ('gabe', 2695),\n",
       " ('stanley', 2644),\n",
       " ('david', 2291),\n",
       " ('holly', 2132),\n",
       " ('meredith', 2128),\n",
       " ('creed', 1924),\n",
       " ('deangelo', 1527),\n",
       " ('jo', 1317),\n",
       " ('karen', 1219),\n",
       " ('clark', 1137),\n",
       " ('packer', 924),\n",
       " ('roy', 850),\n",
       " ('pete', 796)]"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by non-stopword word count\n",
    "sorted(\n",
    "    ((char, count) \n",
    "        for char in season_bows['all'].keys()\n",
    "        if (count := sum(season_bows['all'][char].values()) - season_bows['all'][char]['total_count'])),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:30]"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1667672ef18e094cb4c73dd20551c461edfdadf76f57a095a18e81154bdbad1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('avatar-script-analysis': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
