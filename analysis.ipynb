{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "important_characters = {\n",
    "    'sokka', 'aang', 'katara', 'zuko', 'toph', 'iroh', 'zuko', 'azula', # 1000+ non-stopwords\n",
    "    'zhao', 'jet', 'suki', 'hakoda', 'bumi',                            # 400+ non-stopwords\n",
    "}\n",
    "minor_characters = {\n",
    "    'hama', 'ozai', 'guru pathik', 'mai', 'roku', 'long feng',          # 350+ non-stopwords\n",
    "    'the mechanist', 'piandao', 'warden', 'bato', 'ty lee',\n",
    "    'jeong jeong', 'pakku', 'zhang leader', 'earth king',\n",
    "    'professor zei', 'joo dee', 'chong', 'yue',                         # 200+ non-stopwords\n",
    "    'appa', 'momo'                                                      # :)\n",
    "}\n",
    "aliases = {\n",
    "    'fire lord ozai': 'ozai', 'ruko': 'roku', 'princess yue': 'yue', 'princess yue / moon spirit': 'yue', 'monk gyatso': 'gyatso',\n",
    "    'themechanist': 'the mechanist', 'zuko/blue spirit': 'zuko', 'king bumi': 'bumi', 'master pakku': 'pakku'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def get_ep_character_bows(script_lines):\n",
    "    \"\"\"Return a set of bags of words for this episode as a mapping from characters to words to counts.\"\"\"\n",
    "    bow = defaultdict(Counter)\n",
    "    for line in script_lines:\n",
    "        speaker, line = line.split('::')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        if speaker not in important_characters:\n",
    "            continue\n",
    "        line = line.lower()\n",
    "        # Get all non-stop words\n",
    "        stop_words = set(w for w in stopwords.words('english') \n",
    "            + ['us', 'get', 'like', 'thats', 'go', 'going', 'cant', 'yeh', 'oh', 'got', 'hey', \n",
    "               'yeah', 'uh', 'whats', 'could', 'shall', 'gonna', 'okay', 'one'] \n",
    "        )\n",
    "        words = re.findall(r'\\w+', line)\n",
    "        bow[speaker]['total_count'] += len(words)\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "        bow[speaker].update(words)\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_bows(bows):\n",
    "    \"\"\"Merge a collection of bags of words into one. Does not modify the input.\"\"\"\n",
    "    combined_bow = Counter()\n",
    "    for bow in bows:\n",
    "        combined_bow.update(bow)\n",
    "    return combined_bow\n",
    "\n",
    "def combine_character_bows(character_bows):\n",
    "    \"\"\"Merge a collection of mappings from characters to bags of words into one. Does not modify the input.\"\"\"\n",
    "    all_characters = set(k for bow in character_bows for k in bow.keys())\n",
    "    combined_bow_set = {c: combine_bows(bow_map[c] for bow_map in character_bows) for c in all_characters}\n",
    "    return defaultdict(Counter, combined_bow_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_words_by_bayes(character_bows):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, p), where w is a word \n",
    "    and p is the probability that a random occurrence of the word was spoken by that character.\n",
    "    The lists are sorted by descending probability.\n",
    "    \"\"\"\n",
    "    # Compute the total number of occurrences for each word\n",
    "    combined_bow = combine_bows(character_bows.values())\n",
    "    # p(character | word) = [# times character says word] / [# occurrences of word]\n",
    "    words_by_bayes = {\n",
    "        c: sorted(\n",
    "            [(w, character_bows[c][w] / combined_bow[w]) for w in combined_bow.keys()],\n",
    "            key=lambda wp: wp[1],\n",
    "            reverse=True\n",
    "        ) for c in character_bows.keys()\n",
    "    }\n",
    "    return words_by_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "# Since we have very few characters, we want to weight idf highly (with a lower base)\n",
    "base = 1.2\n",
    "\n",
    "def get_words_by_tfidf(character_bows):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, s), where w is a word \n",
    "    and s is the tf-idf score of that word for that character. The lists are sorted \n",
    "    by descending score.\n",
    "    \"\"\"\n",
    "    # Compute the number of characters that said each word\n",
    "    all_words = set(k for bow in character_bows.values() for k in bow.keys())\n",
    "    n_chars = len(character_bows.keys())\n",
    "    idf = {\n",
    "        w: 1 + log(n_chars / (1 + sum(character_bows[c][w] > 0 for c in character_bows.keys())), base)\n",
    "        for w in all_words\n",
    "    }\n",
    "    # tf-idf = tf(w, c) / (1 + log(N / (1 + df(w))))\n",
    "    words_by_tfidf = {\n",
    "        c: sorted(\n",
    "            [(w, (1 + v) * idf[w]) for w, v in character_bows[c].items()],\n",
    "            key=lambda ws: ws[1],\n",
    "            reverse=True\n",
    "        ) for c in character_bows.keys()\n",
    "    }\n",
    "    return words_by_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_character_sentence_sentiments(script_lines):\n",
    "    \"\"\"\n",
    "    Return a mapping from each character to a list of all their sentences along with the\n",
    "    computed sentiment scores (positive, negative, neutral) for each sentence.\n",
    "    \"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    char_sentence_sentiments = defaultdict(list)\n",
    "    for line in script_lines:\n",
    "        speaker, line = line.split('::')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        if speaker not in important_characters | minor_characters:\n",
    "            continue\n",
    "        \n",
    "        char_sentence_sentiments[speaker].extend(\n",
    "            (sid.polarity_scores(sentence), sentence) for sentence in tokenizer.tokenize(line)\n",
    "        )\n",
    "\n",
    "    return char_sentence_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_character_sentence_sentiments(target_map, added_map):\n",
    "    \"\"\"\n",
    "    Given two maps from characters to lists of sentence sentiments, combine all lists with the same key.\n",
    "    The first argument will be modified.\n",
    "    \"\"\"\n",
    "    for char in added_map.keys():\n",
    "        target_map[char].extend(added_map[char])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characters_by_net_sentiment(char_sentence_sentiments):\n",
    "    \"\"\"\n",
    "    Given a map from characters to lists of sentence sentiments, sort all characters descending\n",
    "    by their net positive sentiment, where positive sentences contribute +1/N, negative sentences\n",
    "    contribute -1/N, and neutral sentences contribute 0.\n",
    "    \"\"\"\n",
    "    net_sentiments = defaultdict(int)\n",
    "    for char, sentiments in char_sentence_sentiments.items():\n",
    "        for (scores, _) in sentiments:\n",
    "            net_sentiments[char] += (1 if scores['compound'] > 0.05 else -1 if scores['compound'] < -0.05 else 0)/len(sentiments)\n",
    "    return sorted(net_sentiments.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "season_bows_collection = defaultdict(list)\n",
    "season_bows = {}\n",
    "season_tfidf = {}\n",
    "char_sentence_sentiments = defaultdict(list)\n",
    "for season in range(1, 4):\n",
    "    for episode in range(1, 21):\n",
    "        ep_num = f'{season}{str(episode).zfill(2)}'\n",
    "        with open(f'transcripts/{ep_num}.txt') as f:\n",
    "            script_lines = f.readlines()\n",
    "            season_bows_collection[season].append(get_ep_character_bows(script_lines))\n",
    "            merge_character_sentence_sentiments(char_sentence_sentiments, get_character_sentence_sentiments(script_lines))\n",
    "    season_bows[season] = combine_character_bows(season_bows_collection[season])\n",
    "    season_tfidf[season] = get_words_by_tfidf(season_bows[season])\n",
    "season_bows['all'] = combine_character_bows([bows for s in range(1, 4) for bows in season_bows_collection[s]])\n",
    "season_tfidf['all'] = get_words_by_tfidf(season_bows['all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[({'neg': 0.137, 'neu': 0.668, 'pos': 0.194, 'compound': 0.2363},\n  \" Just because you're destined to save the world, don't expect any special treatment.\"),\n ({'neg': 0.0, 'neu': 0.656, 'pos': 0.344, 'compound': 0.4939},\n  ' If you want to relax, then I suggest visiting a tropical island.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"If not, I'll see you both at sunrise.\"),\n ({'neg': 0.0, 'neu': 0.256, 'pos': 0.744, 'compound': 0.4404}, 'Good night.'),\n ({'neg': 0.176, 'neu': 0.64, 'pos': 0.184, 'compound': 0.0258},\n  \" No, please, march right in, I'm not concentrating or anything.\"),\n ({'neg': 0.565, 'neu': 0.435, 'pos': 0.0, 'compound': -0.0772},\n  \" I'm sorry.\"),\n ({'neg': 0.483, 'neu': 0.517, 'pos': 0.0, 'compound': -0.4215},\n  \"I think there's been a misunderstanding.\"),\n ({'neg': 0.0, 'neu': 0.686, 'pos': 0.314, 'compound': 0.4939},\n  \"You didn't tell me your friend was a girl.\"),\n ({'neg': 0.219, 'neu': 0.781, 'pos': 0.0, 'compound': -0.4215},\n  'In our tribe, it is forbidden for women to learn waterbending.'),\n ({'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.296}, ' No.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' Here the women learn from Yugoda to use their waterbending to heal.'),\n ({'neg': 0.0, 'neu': 0.595, 'pos': 0.405, 'compound': 0.8338},\n  \"I'm sure she would be happy to take you as her student despite your bad attitude.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' I can see that.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'But our tribe has customs, rules.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Then what?'),\n ({'neg': 0.0, 'neu': 0.357, 'pos': 0.643, 'compound': 0.6597},\n  ' Well have fun teaching yourself.'),\n ({'neg': 0.0, 'neu': 0.385, 'pos': 0.615, 'compound': 0.7506},\n  \"I'm sure you'll do a great job.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \" Why don't we get started then.\"),\n ({'neg': 0.115, 'neu': 0.885, 'pos': 0.0, 'compound': -0.1419},\n  \" You're moving the water around, but you're not feeling the push and pull.\"),\n ({'neg': 0.0, 'neu': 0.778, 'pos': 0.222, 'compound': 0.25},\n  ' Maybe that move is too advanced for you.'),\n ({'neg': 0.0, 'neu': 0.682, 'pos': 0.318, 'compound': 0.4215},\n  \"Why don't you try an easier one?\"),\n ({'neg': 0.25, 'neu': 0.75, 'pos': 0.0, 'compound': -0.4588},\n  ' You have disrespected me, my teachings, and my entire culture.'),\n ({'neg': 0.196, 'neu': 0.536, 'pos': 0.268, 'compound': 0.2023},\n  ' You are no longer welcome as my student.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \" I'm waiting, little girl.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' Go back to the healing huts with the other women where you belong.'),\n ({'neg': 0.0, 'neu': 0.0, 'pos': 1.0, 'compound': 0.2023}, ' Fine.'),\n ({'neg': 0.471, 'neu': 0.446, 'pos': 0.083, 'compound': -0.7954},\n  'You want to learn to fight so bad, study closely!'),\n ({'neg': 0.0, 'neu': 0.523, 'pos': 0.477, 'compound': 0.6677},\n  \" Don't worry, I'm not going to hurt you!\"),\n ({'neg': 0.0, 'neu': 0.161, 'pos': 0.839, 'compound': 0.6369},\n  \" Well, I'm impressed.\"),\n ({'neg': 0.0, 'neu': 0.519, 'pos': 0.481, 'compound': 0.5719},\n  'You are an excellent waterbender.'),\n ({'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.296}, ' No.'),\n ({'neg': 0.464, 'neu': 0.536, 'pos': 0.0, 'compound': -0.3818},\n  ' This fight is over.'),\n ({'neg': 0.0, 'neu': 0.426, 'pos': 0.574, 'compound': 0.4019},\n  ' Yes, you are.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' This is my necklace!'),\n ({'neg': 0.0, 'neu': 0.704, 'pos': 0.296, 'compound': 0.6369},\n  ' I made this sixty years ago â€“ for the love of my life.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, 'For Kana.'),\n ({'neg': 0.0, 'neu': 0.769, 'pos': 0.231, 'compound': 0.4019},\n  ' I carved this necklace for your grandmother when we got engaged.'),\n ({'neg': 0.0, 'neu': 0.654, 'pos': 0.346, 'compound': 0.5719},\n  'I thought we would have a long, happy life together.'),\n ({'neg': 0.0, 'neu': 0.204, 'pos': 0.796, 'compound': 0.5994},\n  'I loved her.'),\n ({'neg': 0.0, 'neu': 0.241, 'pos': 0.759, 'compound': 0.484}, ' Not bad!'),\n ({'neg': 0.0, 'neu': 0.241, 'pos': 0.759, 'compound': 0.484}, 'Not bad!'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, 'Heheh!'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"Keep practicing and maybe you'll get it by the time you're my age!\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \" What do you think you're doing?\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, \"It's past sunrise.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, \"You're late.\"),\n ({'neg': 0.0, 'neu': 0.517, 'pos': 0.483, 'compound': 0.4215},\n  ' Nice try, Pupil Sangok.'),\n ({'neg': 0.161, 'neu': 0.683, 'pos': 0.155, 'compound': -0.0258},\n  'A couple of more years and you might be ready to fight a sea sponge.'),\n ({'neg': 0.0, 'neu': 0.652, 'pos': 0.348, 'compound': 0.4939},\n  ' Would anyone care for a rematch with Katara?'),\n ({'neg': 0.0, 'neu': 0.846, 'pos': 0.154, 'compound': 0.25},\n  ' Katara, you have advanced more quickly than any student I have ever trained.'),\n ({'neg': 0.067, 'neu': 0.526, 'pos': 0.407, 'compound': 0.7964},\n  'You have proven that with fierce determination, passion and hard work you can accomplish anything.'),\n ({'neg': 0.227, 'neu': 0.455, 'pos': 0.318, 'compound': 0.2023},\n  'Raw talent alone is not enough.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Pupil Aang!'),\n ({'neg': 0.0, 'neu': 0.652, 'pos': 0.348, 'compound': 0.4939},\n  ' Care to step into the sparring circle?'),\n ({'neg': 0.0, 'neu': 0.862, 'pos': 0.138, 'compound': 0.34},\n  \"I figure since you've found time to play with house pets you must have already mastered waterbending.\"),\n ({'neg': 0.555, 'neu': 0.445, 'pos': 0.0, 'compound': -0.3595},\n  ' Stop those fireballs!'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \" I've decided to go to the South Pole.\"),\n ({'neg': 0.0, 'neu': 0.667, 'pos': 0.333, 'compound': 0.3612},\n  'Some other benders and healers want to join me.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  \"It's time we helped rebuild our sister tribe.\"),\n ({'neg': 0.0, 'neu': 0.643, 'pos': 0.357, 'compound': 0.6124},\n  ' Well, then he better get used to calling you \"Master Katara.\"'),\n ({'neg': 0.0, 'neu': 0.794, 'pos': 0.206, 'compound': 0.0772},\n  ' Katara, I want you to have this.'),\n ({'neg': 0.0, 'neu': 0.805, 'pos': 0.195, 'compound': 0.1779},\n  'This amulet contains water from the Spirit Oasis.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'The water has unique properties.'),\n ({'neg': 0.0, 'neu': 0.47, 'pos': 0.53, 'compound': 0.3089},\n  \"Don't lose it.\"),\n ({'neg': 0.15, 'neu': 0.751, 'pos': 0.099, 'compound': -0.2382},\n  \" Aang, these scrolls will help you master waterbending, but remember they're no substitute for a real master.\"),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}, ' Sokka.'),\n ({'neg': 0.0, 'neu': 0.385, 'pos': 0.615, 'compound': 0.4939},\n  'Take care, son.'),\n ({'neg': 0.0, 'neu': 0.853, 'pos': 0.147, 'compound': 0.2263},\n  ' Fly straight to the Earth Kingdom base to the east of here.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  'General Fong will provide you with an escort to Omashu.'),\n ({'neg': 0.0, 'neu': 0.791, 'pos': 0.209, 'compound': 0.4404},\n  \"There you'll be safe to begin your earthbending training with King Bumi.\"),\n ({'neg': 0.0, 'neu': 0.709, 'pos': 0.291, 'compound': 0.7311},\n  '  It is respectful to bow to an old master but how about a hug  for your new grandfather?'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' I made her a  new betrothal necklace and everything.'),\n ({'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0},\n  ' You can still just call me  Pakku.'),\n ({'neg': 1.0, 'neu': 0.0, 'pos': 0.0, 'compound': -0.296}, '  No.'),\n ({'neg': 0.156, 'neu': 0.649, 'pos': 0.195, 'compound': 0.1531},\n  '  It came from a Grand Lotus, your Uncle,  Iroh of the Fire Nation.'),\n ({'neg': 0.0, 'neu': 0.548, 'pos': 0.452, 'compound': 0.5106},\n  ' Here to set you free.')]"
     },
     "metadata": {},
     "execution_count": 68
    }
   ],
   "source": [
    "char_sentence_sentiments['ty lee']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('ty lee', 0.38383838383838403),\n ('pakku', 0.3552631578947366),\n ('professor zei', 0.2622950819672131),\n ('iroh', 0.18390804597701185),\n ('bumi', 0.17333333333333326),\n ('piandao', 0.1645569620253165),\n ('guru pathik', 0.15463917525773196),\n ('yue', 0.15217391304347822),\n ('joo dee', 0.13793103448275865),\n ('suki', 0.11398963730569951),\n ('roku', 0.1111111111111111),\n ('toph', 0.10926365795724474),\n ('aang', 0.1059624108878811),\n ('chong', 0.1044776119402985),\n ('bato', 0.09090909090909093),\n ('earth king', 0.09090909090909091),\n ('hama', 0.08080808080808081),\n ('azula', 0.07707910750507099),\n ('hakoda', 0.05797101449275362),\n ('katara', 0.055412371134020484),\n ('ozai', 0.052173913043478265),\n ('zuko', 0.034746351633078515),\n ('sokka', 0.02602739726027394),\n ('jet', 0.019801980198019802),\n ('momo', 0.0),\n ('zhao', -0.0179372197309417),\n ('warden', -0.01818181818181818),\n ('mai', -0.023076923076923078),\n ('jeong jeong', -0.07317073170731707),\n ('long feng', -0.07368421052631578),\n ('the mechanist', -0.08247422680412371),\n ('zhang leader', -0.15217391304347827),\n ('appa', -0.2857142857142857)]"
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "source": [
    "get_characters_by_net_sentiment(char_sentence_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "[('father', 95.04460042309827),\n ('uncle', 79.8199937280543),\n ('mai', 68.43211230463076),\n ('raiders', 58.964814717536434),\n ('stone', 49.13734559794703),\n ('changed', 45.62140820308717),\n ('dragon', 45.62140820308717),\n ('dragons', 45.62140820308717),\n ('history', 42.17979571865833),\n ('meeting', 42.17979571865833),\n ('firebending', 41.38814489602815),\n ('killed', 39.30987647835762),\n ('accepted', 39.30987647835762),\n ('sages', 39.30987647835762)]"
     },
     "metadata": {},
     "execution_count": 55
    }
   ],
   "source": [
    "season_tfidf[3]['zuko'][1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "season_tfidf['all']['suki'][1:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(\n",
    "    ((char, count) \n",
    "        for char in season_bows['all'].keys()\n",
    "        if (count := season_bows['all'][char]['total_count'])),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python38364bitavatarscriptanalysispipenv8de1ba2f463a4d24ae2f4934f1108141",
   "display_name": "Python 3.8.3 64-bit ('avatar-script-analysis': pipenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}