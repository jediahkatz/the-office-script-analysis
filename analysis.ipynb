{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "major_characters = {\n",
    "    'sokka', 'aang', 'katara', 'zuko', 'toph', 'iroh', 'zuko', 'azula', # 1000+ non-stopwords / 3000+ total words\n",
    "    'zhao', 'jet', 'suki', 'hakoda', 'bumi',                            # 400+ non-stopwords / 1000+ total words\n",
    "}\n",
    "minor_characters = {\n",
    "    'hama', 'ozai', 'guru pathik', 'mai', 'roku', 'long feng',          # 350+ non-stopwords\n",
    "    'mechanist', 'piandao', 'warden', 'bato', 'ty lee',\n",
    "    'jeong jeong', 'pakku', 'earth king', # 'zhang leader'\n",
    "    'professor zei', 'joo dee', 'chong', 'yue',                         # 200+ non-stopwords\n",
    "    'appa', 'momo'                                                      # :)\n",
    "}\n",
    "major_characters = minor_characters = None\n",
    "aliases = {\n",
    "    # 'fire lord ozai': 'ozai', 'ruko': 'roku', 'princess yue': 'yue', 'princess yue / moon spirit': 'yue', 'monk gyatso': 'gyatso',\n",
    "    # 'themechanist': 'mechanist', 'the mechanist': 'mechanist', 'zuko/blue spirit': 'zuko', 'king bumi': 'bumi', 'master pakku': 'pakku'\n",
    "}\n",
    "word_substitutions = {\n",
    "    # 'jeong jeong': 'jeong_jeong', 'long feng': 'long_feng', 'ty lee': 'ty_lee', 'joo dee': 'joo_dee', 'wan shi tong': 'wan_shi_tong', \n",
    "    # 'chit sang': 'chit_sang', 'ba sing se': 'ba_sing_se', 'twinkle toes': 'twinkletoes', 'twinkle-toes': 'twinkletoes', 'mum': 'mom',\n",
    "    # 'dai li': 'dai_li', 'sozen': 'sozin', 'honour': 'honor'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from functools import reduce\n",
    "import re\n",
    "\n",
    "def get_ep_character_bows(script_lines):\n",
    "    \"\"\"Return a set of bags of words for this episode as a mapping from characters to words to counts.\"\"\"\n",
    "    bow = defaultdict(Counter)\n",
    "    for line in script_lines:\n",
    "        try:\n",
    "            speaker, line = line.split('::')\n",
    "        except:\n",
    "            print(f'line: <<{line}>>')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        line = line.lower()\n",
    "        # Get all non-stop words\n",
    "        stop_words = set(w for w in stopwords.words('english') \n",
    "            + ['us', 'get', 'like', 'thats', 'go', 'going', 'cant', 'oh', 'got', 'hey', 'would',\n",
    "               'yeah', 'yeh', 'ya', 'uh', 'whats', 'could', 'shall', 'gonna', 'okay', 'one', \n",
    "               'something', 'may'] \n",
    "        )\n",
    "        # Special cases, typically multi-word names treated as one word\n",
    "        words = [w.replace('_', ' ') for w in\n",
    "            re.findall(r'\\w+', \n",
    "                reduce(lambda acc, k: acc.replace(k, word_substitutions[k]), word_substitutions.keys(), line)\n",
    "            )]\n",
    "        bow[speaker]['total_count'] += len(words)\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "        bow[speaker].update(words)\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ep_total_word_count(script_lines):\n",
    "    return sum(len(re.findall(r'\\w+', line.split('::')[1])) for line in script_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_bows(bows):\n",
    "    \"\"\"Merge a collection of bags of words into one. Does not modify the input.\"\"\"\n",
    "    combined_bow = Counter()\n",
    "    for bow in bows:\n",
    "        combined_bow.update(bow)\n",
    "    return combined_bow\n",
    "\n",
    "def combine_character_bows(character_bows):\n",
    "    \"\"\"Merge a collection of mappings from characters to bags of words into one. Does not modify the input.\"\"\"\n",
    "    all_characters = set(k for bow in character_bows for k in bow.keys())\n",
    "    combined_bow_set = {c: combine_bows(bow_map[c] for bow_map in character_bows) for c in all_characters}\n",
    "    return defaultdict(Counter, combined_bow_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: this is a bad metric -- it just finds words that this character said once and nobody else ever said.\n",
    "def get_words_by_bayes(character_bows):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, p), where w is a word \n",
    "    and p is the probability that a random occurrence of the word was spoken by that character.\n",
    "    The lists are sorted by descending probability.\n",
    "    \"\"\"\n",
    "    # Compute the total number of occurrences for each word\n",
    "    combined_bow = combine_bows(character_bows.values())\n",
    "    # p(character | word) = [# times character says word] / [# occurrences of word]\n",
    "    words_by_bayes = {\n",
    "        c: sorted(\n",
    "            [(w, character_bows[c][w] / combined_bow[w]) for w in combined_bow.keys()],\n",
    "            key=lambda wp: wp[1],\n",
    "            reverse=True\n",
    "        ) for c in character_bows.keys()\n",
    "    }\n",
    "    return words_by_bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "# Since we have very few characters, we want to weight idf highly (with a lower base)\n",
    "base = 1.2\n",
    "\n",
    "# I experimented with these parameters but ultimately decided they were more harmful than helpful on such a small dataset\n",
    "def get_words_by_tfidf(character_bows, character_set=None, sublinear_tf=False, filter_df_1=False):\n",
    "    \"\"\"\n",
    "    Given a mapping from characters to bags of words (for an episode, season, etc), \n",
    "    return a mapping from each character to a list of pairs (w, s), where w is a word \n",
    "    and s is the tf-idf score of that word for that character. The lists are sorted \n",
    "    by descending score.\n",
    "    \"\"\"\n",
    "    # If no explicit character set is passed, just use all characters\n",
    "    character_set = character_set or character_bows.keys()\n",
    "    # Compute the number of characters that said each word\n",
    "    all_words = set(k for bow in character_bows.values() for k in bow.keys())\n",
    "    n_chars = len(character_set)\n",
    "    idf = {\n",
    "        w: 1 + log(n_chars / (1 + df), base)\n",
    "        for w in all_words if (df := sum(character_bows[c][w] > 0 for c in character_set)) > (1 if filter_df_1 else 0)\n",
    "    }\n",
    "    # tf-idf = tf(w, c) / (1 + log(N / (1 + df(w))))\n",
    "    words_by_tfidf = {\n",
    "        c: sorted([\n",
    "                (w, ((1 + log(v)) if sublinear_tf else v) * idf[w]) \n",
    "                for w, v in character_bows[c].items() \n",
    "                if w != 'total_count' and (not filter_df_1 or w in idf)\n",
    "            ],\n",
    "            key=lambda ws: ws[1],\n",
    "            reverse=True\n",
    "        ) for c in character_set\n",
    "    }\n",
    "    return words_by_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk.data\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk import sentiment\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def get_character_sentence_sentiments(script_lines, character_set=None):\n",
    "    \"\"\"\n",
    "    Return a mapping from each character to a list of all their sentences along with the\n",
    "    computed sentiment scores (positive, negative, neutral) for each sentence.\n",
    "    \"\"\"\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "    char_sentence_sentiments = defaultdict(list)\n",
    "    for line in script_lines:\n",
    "        speaker, line = line.split('::')\n",
    "        speaker = speaker.lower().strip()\n",
    "        speaker = aliases.get(speaker, speaker)\n",
    "        if character_set and speaker not in character_set:\n",
    "            continue\n",
    "        \n",
    "        char_sentence_sentiments[speaker].extend(\n",
    "            (sid.polarity_scores(sentence), sentence) for sentence in tokenizer.tokenize(line)\n",
    "        )\n",
    "\n",
    "    return char_sentence_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_character_sentence_sentiments(target_map, added_map):\n",
    "    \"\"\"\n",
    "    Given two maps from characters to lists of sentence sentiments, combine all lists with the same key.\n",
    "    The first argument will be modified.\n",
    "    \"\"\"\n",
    "    for char in added_map.keys():\n",
    "        target_map[char].extend(added_map[char])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_character_sentiment_frequencies(char_sentence_sentiments):\n",
    "    \"\"\"\n",
    "    Given a map from characters to lists of sentence sentiments, return a map\n",
    "    from characters to lists of the frequency of positive, negative, and neutral\n",
    "    sentiments.\n",
    "    \"\"\"\n",
    "    sentiment_freqs = defaultdict(lambda: defaultdict(int))\n",
    "    for char, sentiments in char_sentence_sentiments.items():\n",
    "        for (scores, _) in sentiments:\n",
    "            sentiment_freqs[char][\n",
    "                'pos' if scores['compound'] > 0.05 else 'neg' if scores['compound'] < -0.05 else 'neu'\n",
    "            ] += 1/len(sentiments)\n",
    "    return sentiment_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ep in listdir('transcripts/'):\n",
    "    lines =[]\n",
    "    with open(f'transcripts/{ep}') as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [l for l in lines if '::' in l]\n",
    "    with open(f'transcripts/{ep}', 'w') as f:\n",
    "        f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line: <<In memory of Larry Einhorn.\n",
      ">>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb Cell 10'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000009vscode-remote?line=14'>15</a>\u001b[0m         script_lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000009vscode-remote?line=15'>16</a>\u001b[0m         season_bows_collection[season]\u001b[39m.\u001b[39mappend(get_ep_character_bows(script_lines))\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000009vscode-remote?line=16'>17</a>\u001b[0m         merge_character_sentence_sentiments(character_sentence_sentiments, get_character_sentence_sentiments(script_lines))\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000009vscode-remote?line=17'>18</a>\u001b[0m         total_word_count[episode] \u001b[39m=\u001b[39m get_ep_total_word_count(script_lines)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000009vscode-remote?line=18'>19</a>\u001b[0m season_bows[season] \u001b[39m=\u001b[39m combine_character_bows(season_bows_collection[season])\n",
      "\u001b[1;32m/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb Cell 7'\u001b[0m in \u001b[0;36mget_character_sentence_sentiments\u001b[0;34m(script_lines, character_set)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000006vscode-remote?line=13'>14</a>\u001b[0m char_sentence_sentiments \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000006vscode-remote?line=14'>15</a>\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m script_lines:\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000006vscode-remote?line=15'>16</a>\u001b[0m     speaker, line \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m::\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000006vscode-remote?line=16'>17</a>\u001b[0m     speaker \u001b[39m=\u001b[39m speaker\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstrip()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000006vscode-remote?line=17'>18</a>\u001b[0m     speaker \u001b[39m=\u001b[39m aliases\u001b[39m.\u001b[39mget(speaker, speaker)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "\n",
    "season_bows_collection = defaultdict(list)\n",
    "season_bows = {}\n",
    "season_tfidf = {}\n",
    "season_tfidf_minor = {}\n",
    "season_bayes = {}\n",
    "total_word_count = {}\n",
    "character_sentence_sentiments = defaultdict(list)\n",
    "\n",
    "episodes = [ep.replace('.txt', '') for ep in listdir('transcripts/')]\n",
    "for season in range(1, 10):\n",
    "    for episode in (e for e in episodes if f'0{season}x' in e):\n",
    "        with open(f'transcripts/{episode}.txt') as f:\n",
    "            script_lines = f.readlines()\n",
    "            season_bows_collection[season].append(get_ep_character_bows(script_lines))\n",
    "            merge_character_sentence_sentiments(character_sentence_sentiments, get_character_sentence_sentiments(script_lines))\n",
    "            total_word_count[episode] = get_ep_total_word_count(script_lines)\n",
    "    season_bows[season] = combine_character_bows(season_bows_collection[season])\n",
    "    # It helps to compute tf-idf scores for major characters only (excluding minor characters) or\n",
    "    # else depth-of-character words like \"know\", \"want\", \"think\" are weighted too highly\n",
    "    season_tfidf[season] = get_words_by_tfidf(season_bows[season], character_set=major_characters)\n",
    "    season_tfidf_minor[season] = get_words_by_tfidf(season_bows[season], character_set=None) #major_characters | minor_characters)\n",
    "    season_bayes[season] = get_words_by_bayes(season_bows[season])\n",
    "season_bows['all'] = combine_character_bows([bows for s in range(1, 10) for bows in season_bows_collection[s]])\n",
    "season_tfidf['all'] = get_words_by_tfidf(season_bows['all'], character_set=major_characters)\n",
    "season_tfidf_minor['all'] = get_words_by_tfidf(season_bows['all'], character_set=major_characters | minor_characters)\n",
    "season_bayes['all'] = get_words_by_bayes(season_bows['all'])\n",
    "character_sentiment_frequencies = get_character_sentiment_frequencies(character_sentence_sentiments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'all'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb Cell 12'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000010vscode-remote?line=0'>1</a>\u001b[0m \u001b[39m# Sort by total word count\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000010vscode-remote?line=1'>2</a>\u001b[0m count_sorted \u001b[39m=\u001b[39m \u001b[39msorted\u001b[39m(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000010vscode-remote?line=2'>3</a>\u001b[0m     ((char, count) \n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000010vscode-remote?line=3'>4</a>\u001b[0m         \u001b[39mfor\u001b[39;00m char \u001b[39min\u001b[39;00m season_bows[\u001b[39m'\u001b[39;49m\u001b[39mall\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39mkeys()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000010vscode-remote?line=4'>5</a>\u001b[0m         \u001b[39mif\u001b[39;00m (count \u001b[39m:=\u001b[39m season_bows[\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m][char][\u001b[39m'\u001b[39m\u001b[39mtotal_count\u001b[39m\u001b[39m'\u001b[39m])),\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000010vscode-remote?line=5'>6</a>\u001b[0m     key\u001b[39m=\u001b[39m\u001b[39mlambda\u001b[39;00m x: x[\u001b[39m1\u001b[39m],\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000010vscode-remote?line=6'>7</a>\u001b[0m     reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000010vscode-remote?line=7'>8</a>\u001b[0m )[:\u001b[39m30\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'all'"
     ]
    }
   ],
   "source": [
    "# Sort by total word count\n",
    "count_sorted = sorted(\n",
    "    ((char, count) \n",
    "        for char in season_bows['all'].keys()\n",
    "        if (count := season_bows['all'][char]['total_count'])),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for |: 'NoneType' and 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000011vscode-remote?line=3'>4</a>\u001b[0m count_json \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000011vscode-remote?line=4'>5</a>\u001b[0m sentiment_json \u001b[39m=\u001b[39m defaultdict(\u001b[39mlist\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000011vscode-remote?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(major_characters \u001b[39m|\u001b[39;49m minor_characters):\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000011vscode-remote?line=6'>7</a>\u001b[0m     ct \u001b[39m=\u001b[39m c\u001b[39m.\u001b[39mtitle()\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu-20.04/mnt/c/Users/jedia/Dropbox/College-Docs/NETS-150/the-office-script-analysis/analysis.ipynb#ch0000011vscode-remote?line=7'>8</a>\u001b[0m     \u001b[39mfor\u001b[39;00m season \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mall\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m3\u001b[39m]:\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for |: 'NoneType' and 'NoneType'"
     ]
    }
   ],
   "source": [
    "from json import dump\n",
    "\n",
    "words_json = defaultdict(dict)\n",
    "count_json = defaultdict(list)\n",
    "sentiment_json = defaultdict(list)\n",
    "for c in sorted(major_characters | minor_characters):\n",
    "    ct = c.title()\n",
    "    for season in ['all', 1, 2, 3]:\n",
    "        top_ten = [\n",
    "            w for (w, _) in (\n",
    "                season_tfidf[season][c][:10] if c in major_characters else season_tfidf_minor[season][c][:10]\n",
    "            )\n",
    "        ]\n",
    "        words_json[ct][season] = top_ten\n",
    "    words_json[ct]['word_count'] = season_bows['all'][c]['total_count']\n",
    "    words_json[ct]['neg_freq'] = character_sentiment_frequencies[c]['neg']\n",
    "    words_json[ct]['neu_freq'] = character_sentiment_frequencies[c]['neu']\n",
    "    words_json[ct]['pos_freq'] = character_sentiment_frequencies[c]['pos']\n",
    "    words_json[ct]['net_freq'] = character_sentiment_frequencies[c]['pos'] - character_sentiment_frequencies[c]['neg']\n",
    "\n",
    "for (c , wc) in count_sorted[:10]:\n",
    "    for season in range(1, 4):\n",
    "        count_json[f'S{season}'].append(season_bows[season][c]['total_count'])\n",
    "    count_json['characters'].append(c.title())\n",
    "\n",
    "# Arbitrarily selected list of characters to highlight for sentiment analysis\n",
    "for c in ['ty lee', 'iroh', 'toph', 'aang', 'azula', 'katara', 'zuko', 'sokka', 'mai', 'long feng']:\n",
    "    sentiment_json['characters'].append(c.title())\n",
    "    sentiment_json['pos'].append(character_sentiment_frequencies[c]['pos'])\n",
    "    sentiment_json['neg'].append(-character_sentiment_frequencies[c]['neg'])\n",
    "\n",
    "with open('results.json', 'w') as f:\n",
    "    dump([words_json, count_json, sentiment_json], f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sokka', 7666),\n",
       " ('aang', 7337),\n",
       " ('katara', 5985),\n",
       " ('zuko', 3587),\n",
       " ('iroh', 2317),\n",
       " ('toph', 2164),\n",
       " ('azula', 1517),\n",
       " ('zhao', 729),\n",
       " ('jet', 562),\n",
       " ('suki', 480),\n",
       " ('hakoda', 464),\n",
       " ('bumi', 458),\n",
       " ('hama', 383),\n",
       " ('guru pathik', 380),\n",
       " ('ozai', 380),\n",
       " ('roku', 349),\n",
       " ('mai', 345),\n",
       " ('long feng', 324),\n",
       " ('mechanist', 322),\n",
       " ('piandao', 303),\n",
       " ('warden', 294),\n",
       " ('bato', 275),\n",
       " ('ty lee', 267),\n",
       " ('pakku', 262),\n",
       " ('jeong jeong', 247),\n",
       " ('zhang leader', 231),\n",
       " ('earth king', 217),\n",
       " ('chong', 214),\n",
       " ('professor zei', 207),\n",
       " ('yue', 203)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by non-stopword word count\n",
    "sorted(\n",
    "    ((char, count) \n",
    "        for char in season_bows['all'].keys()\n",
    "        if (count := sum(season_bows['all'][char].values()) - season_bows['all'][char]['total_count'])),\n",
    "    key=lambda x: x[1],\n",
    "    reverse=True\n",
    ")[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e1667672ef18e094cb4c73dd20551c461edfdadf76f57a095a18e81154bdbad1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('avatar-script-analysis': pipenv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
